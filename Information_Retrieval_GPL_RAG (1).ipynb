{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5VcHOInjGRg"
      },
      "source": [
        "#An efficient approach to adapt existing dense retrieval models to new domains & data without the need for labelled data\n",
        "\n",
        "# I am not illustrating the GPL training part here. This is a retriever generator pipeline where I am comparing pretrained and GPL trained retriever. Idea is to compare pretrained and GPL trained model for information retrieval over multiple text documents. I am using Pinecone to store vectors and LFQA as generator.\n",
        "\n",
        "\n",
        "#Next steps involve context based chunking to further improve retrieval performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "La0Oegi07gEY",
        "outputId": "45a72e5f-3457-48ea-f440-2b6746e6eecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0X1-GX8egqb"
      },
      "source": [
        "# Necessary Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrUVjXVCjeoO"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util, InputExample, losses\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import CrossEncoder\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import tqdm\n",
        "import random\n",
        "#from datasets import load_dataset\n",
        "import pinecone\n",
        "import opendatasets as od"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deccTNX-ebq7"
      },
      "source": [
        "# Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kv3-0UNbgAb0"
      },
      "outputs": [],
      "source": [
        "# we load old (pretrained) and new (GPL trained) models\n",
        "from sentence_transformers import SentenceTransformer\n",
        "MODEL_PATH = '/content/drive/MyDrive/Astrophysics/biencoder-spacesciencemodel-gtebase' # GPL trained on aviation docs\n",
        "\n",
        "old_model_name = \"msmarco-distilbert-base-tas-b\"\n",
        "old_model = SentenceTransformer(old_model_name)\n",
        "\n",
        "new_model_name = MODEL_PATH\n",
        "new_model = SentenceTransformer(new_model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBHn6hvI4xuc"
      },
      "source": [
        "We load the TAS-B model, a state-of-the-art model trained on MS MARCO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m060ea7PjoNj"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 256\n",
        "model_name = \"msmarco-distilbert-base-tas-b\"\n",
        "\n",
        "org_model = SentenceTransformer(model_name)\n",
        "org_model.max_seq_length = max_seq_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxNJgFhflYiQ"
      },
      "source": [
        "# PineCone Operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYaUQ4Pt4mz0"
      },
      "source": [
        "Logging into Pinecone and creating index negative-mines-petal if not present"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7I3zbXA8YSe"
      },
      "source": [
        "Uploading current documents to Pinecone in batch with a namespace to perform faster search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3wOM1Bj4Zoq"
      },
      "source": [
        "Function to query top 3 similar embeddings from Pinecone, we take the query and get the embeddings based on the model provided. For all the matches, we get the ID and the TEXT from the metadata. This is then convert to a result dictionary to be used for Haystack using the document store."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypj4X3eneq1J"
      },
      "source": [
        "PineCone Wrapper to perform operations such as upload, query and delete for our needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-hrxLfcXvqU"
      },
      "outputs": [],
      "source": [
        "class PineconeWrapper():\n",
        "    def __init__(self, model, top_k=1, dimensions=768):\n",
        "        pinecone.init(api_key='b368dee5-cb6b-4ddf-8858-abcc30c4acb7', environment='us-west1-gcp')\n",
        "        # create new mining index if does not exist\n",
        "        if 'negative-mines-petal' not in pinecone.list_indexes():\n",
        "            pinecone.create_index(\n",
        "                'negative-mines-petal', dimension=dimensions,\n",
        "                metric='dotproduct', pods=1, pod_type='p1'  # limit of pods=1 for free plan (more pods == faster mining)\n",
        "            )\n",
        "        # connect\n",
        "        self.index = pinecone.Index('negative-mines-petal')\n",
        "        print(\"Index Stats: \", self.index)\n",
        "\n",
        "        self.batch_size = 16\n",
        "        self.dimension = dimensions\n",
        "        self.model = model\n",
        "        self.top_k = top_k\n",
        "\n",
        "\n",
        "    def upload_pinecone(self, haystack_docs, namespace):\n",
        "        docs = [haystack_docs[i]['content'] for i in range(len(haystack_docs))]\n",
        "\n",
        "        # doc_dir = [haystack_docs[i]['meta']['doc_dir'] for i in range(len(haystack_docs))]\n",
        "\n",
        "        print(\"\\nUPLOAD PINECONE START!*******!\")\n",
        "        docs_emb = self.model.encode(docs, convert_to_tensor=True, show_progress_bar=True)\n",
        "        print(f\"Document Embeddings Shape : {docs_emb.shape}\")\n",
        "\n",
        "        index_data = self.index.describe_index_stats()\n",
        "        print(f\"Index Data before Adding : {index_data}\")\n",
        "        totalVectorCount = int(index_data['total_vector_count'])\n",
        "\n",
        "        for i in tqdm.tqdm(range(0, len(docs_emb), self.batch_size)):\n",
        "            i_end = min(i+self.batch_size, len(docs_emb))\n",
        "            batch_emb = docs_emb[i:i_end, :].tolist()\n",
        "            # batch_data = docs[i:i_end]\n",
        "\n",
        "            # batch_metadata = [{\"text\": batch_data[i]} for i in range(0, len(batch_data))]\n",
        "            batch_metadata = [{\"text\": haystack_docs[j]['content'],\n",
        "                               \"doc_dir\": haystack_docs[j]['meta']['doc_dir']}\n",
        "                              for j in range(i, i_end)]\n",
        "\n",
        "            batch_ids = [str(x+totalVectorCount) for x in range(i, i_end)]\n",
        "            # print(f\"Batch ID : {batch_ids}, Batch MetaData : {batch_metadata}\")\n",
        "            # print(f\"Batch ID : {batch_ids}, Batch Embeddings : {batch_emb}\")\n",
        "            # print(f\"Batch ID : {batch_ids}, Batch Data : {batch_data}\")\n",
        "            # upload to index\n",
        "            upload_vectors = list(zip(batch_ids, batch_emb, batch_metadata))\n",
        "            print(f\"\\nBatch Upload Vectors : {upload_vectors}\\n\")\n",
        "            self.index.upsert(vectors=upload_vectors, namespace=namespace)\n",
        "\n",
        "        index_data = self.index.describe_index_stats()\n",
        "        print(f\"Index Data after Adding : {index_data}\")\n",
        "        print(\"\\nUPLOAD PINECONE END!*******!\")\n",
        "\n",
        "    def delete_vectors(self, namespace=None):\n",
        "        if not namespace:\n",
        "            index_data = self.index.describe_index_stats()\n",
        "            print(f\"Index Data before deleting : {index_data}\")\n",
        "\n",
        "            to_delete = []\n",
        "            for i in range(12263, 12294):\n",
        "                to_delete.append(str(i))\n",
        "            print(f\"Index to Delete : {to_delete}\")\n",
        "            self.index.delete(ids = to_delete)\n",
        "\n",
        "            index_data = self.index.describe_index_stats()\n",
        "            print(f\"Index Data after deleting : {index_data}\")\n",
        "        else:\n",
        "            index_data = self.index.describe_index_stats()\n",
        "            print(f\"Index Data before deleting : {index_data}\")\n",
        "\n",
        "            self.index.delete(delete_all=True, namespace=namespace)\n",
        "\n",
        "            index_data = self.index.describe_index_stats()\n",
        "            print(f\"Index Data after deleting : {index_data}\")\n",
        "\n",
        "    def query_pinecone(self, query, namespace):\n",
        "        query_emb = self.model.encode(query).tolist()\n",
        "        # print(query_emb.shape)\n",
        "        # print(query_emb)\n",
        "\n",
        "        # res = index.query([query_emb], top_k = 10)\n",
        "        if namespace:\n",
        "            res = self.index.query([query_emb], top_k = self.top_k, namespace=namespace, include_metadata=True)\n",
        "        else:\n",
        "            res = self.index.query([query_emb], top_k = self.top_k)\n",
        "        print(f\"Pinecone Results : {res}\")\n",
        "\n",
        "        # ids = [match.id for match in res['results'][0]['matches']]\n",
        "        # scores = [match.score for match in res['results'][0]['matches']]\n",
        "\n",
        "        # print(ids)\n",
        "        # print(scores)\n",
        "        answers = []\n",
        "\n",
        "        for match in res['matches']:\n",
        "            vector_id = int(match.id)\n",
        "            score = match.score\n",
        "            # text = corpus[vector_id]\n",
        "            try:\n",
        "                text = match['metadata']['text']\n",
        "                doc_dir = match['metadata']['doc_dir']\n",
        "            except:\n",
        "                text = abstracts[vector_id]\n",
        "                doc_dir = \"NA\"\n",
        "\n",
        "            result_dict = {\n",
        "                \"content\" : text,\n",
        "                \"context-type\" : \"text\",\n",
        "                \"meta\":{\n",
        "                    \"id\" : vector_id,\n",
        "                    \"score\": score,\n",
        "                    \"doc_dir\" : doc_dir\n",
        "                }\n",
        "            }\n",
        "            answers.append(result_dict)\n",
        "            # print(f\"Score : {score}, ID : {vector_id}, TEXT : {text}\")\n",
        "\n",
        "        return answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIy81FJftvEW"
      },
      "source": [
        "# LXML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW5YI9bfM44F"
      },
      "source": [
        "# Integrating LFQA and Haystack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhMV6-zMe45M"
      },
      "source": [
        "# Get Answers from LFQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Bq4jnOdM44G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import haystack\n",
        "from haystack import Document\n",
        "#from haystack.generator.transformers import Seq2SeqGenerator\n",
        "from haystack.nodes import Seq2SeqGenerator\n",
        "from haystack.utils import print_answers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajlBYnTNEAEr"
      },
      "source": [
        "Custom function to retrieve documents from input file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDR8xNcpM44J"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "def get_doc(all_doc_dir):\n",
        "    threshold = 300\n",
        "    doc_results = []\n",
        "\n",
        "    print(f\"Documents Found : {os.listdir(all_doc_dir)}\")\n",
        "\n",
        "    for document in os.listdir(all_doc_dir):\n",
        "        doc_dir = os.path.join(all_doc_dir, document)\n",
        "        print(f\"Processing Document : {doc_dir}\")\n",
        "        text = Path(doc_dir).read_text().replace(\"\\n\", \" \")\n",
        "        text = text.replace('\"', \"\")\n",
        "        out = []\n",
        "        doc_results.append(result_dict)\n",
        "\n",
        "    return doc_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNreeniYc4KX"
      },
      "outputs": [],
      "source": [
        "def get_answers_wrapper(query, doc_dir, model, namespace, top_k):\n",
        "    doc_results = get_doc(doc_dir)\n",
        "    # print(f\"DOC RESULTS : {doc_results}\")\n",
        "\n",
        "    pineconeWrapper = PineconeWrapper(model, top_k=top_k, dimensions = 768)\n",
        "\n",
        "    pineconeWrapper.upload_pinecone(doc_results, namespace)\n",
        "\n",
        "    biencoder_results = pineconeWrapper.query_pinecone(query, namespace)\n",
        "    document_store = []\n",
        "\n",
        "    data = {}\n",
        "    data['query'] = query\n",
        "\n",
        "    for idx, doc in enumerate(biencoder_results):\n",
        "        i = str(idx)\n",
        "        data[\"top_\"+i+\"_content\"] = doc['content']\n",
        "        data[\"top_\"+i+\"_dotscore\"] = doc['meta']['score']\n",
        "        data[\"top_\"+i+\"_document\"] = doc['meta']['doc_dir']\n",
        "        document_store.append(Document(doc['content']))\n",
        "\n",
        "    print(f\"All Documents : {document_store}\")\n",
        "\n",
        "    generator = Seq2SeqGenerator(model_name_or_path=\"vblagoje/bart_lfqa\")\n",
        "\n",
        "    result = generator.predict(\n",
        "        query=query,\n",
        "        documents = document_store,\n",
        "        top_k=1\n",
        "    )\n",
        "\n",
        "    print_answers(result, details=\"minimum\")\n",
        "\n",
        "    answers = result['answers']\n",
        "    for answer in answers:\n",
        "        final_ans = answer.answer\n",
        "        ans_score = answer.score\n",
        "        break\n",
        "\n",
        "    data['answer'] = final_ans\n",
        "    data['score'] = ans_score\n",
        "    # return final_ans\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zygLF6l5M44L"
      },
      "outputs": [],
      "source": [
        "def run_inference_refactored(query, doc_dir, model, top_k=5):\n",
        "    pineconeWrapper = PineconeWrapper(model, top_k=top_k)\n",
        "    pineconeWrapper.delete_vectors(namespace='artemis')\n",
        "    data = get_answers_wrapper(query, doc_dir, model, namespace='artemis', top_k=top_k)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjVcAaYs4P20"
      },
      "source": [
        "Specify query and document directory to use the file. Then we get the list of texts from the document. Now we pass the model and the query to top 3 documents from the query_pinecone function. Combined result contains the document from the texts as well as top 3 documents from query_pinecone function. For all the documents in combined results we create a document store. Then we pass this document for the LFQA to obtain the answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1dQe22bq-fH"
      },
      "source": [
        "# CSV Export"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTLxSQ0qEBVk"
      },
      "source": [
        "Function to return for a list of query_mapping values."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
        "\n",
        "old_model_name = \"msmarco-distilbert-base-tas-b\"\n",
        "old_model = SentenceTransformer(old_model_name)\n",
        "MODEL_PATH='/content/drive/MyDrive/Astrophysics/biencoder-spacesciencemodel-gtebase'\n",
        "#MODEL_PATH='/content/drive/MyDrive/Petal/biencoder-arxiv-v2-petal'\n",
        "\n",
        "# new_model_name = \"/content/notebooks/biencoder-arxiv\"\n",
        "new_model_name = MODEL_PATH\n",
        "new_model = SentenceTransformer(new_model_name)"
      ],
      "metadata": {
        "id": "LL-tSXsrNNdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxunjHc3orTe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_csv(queries, all_doc_dir):\n",
        "    all_data = []\n",
        "    for query in queries:\n",
        "        answers_new = run_inference_refactored(query, all_doc_dir, new_model, top_k=3)\n",
        "        answers_new['model_type'] = 'trained'\n",
        "        all_data.append(answers_new)\n",
        "\n",
        "        answers_old = run_inference_refactored(query, all_doc_dir, old_model, top_k=3)\n",
        "        answers_old['model_type'] = 'pretrained'\n",
        "        all_data.append(answers_old)\n",
        "\n",
        "    df = pd.DataFrame(all_data)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1FYtrddq-hX"
      },
      "source": [
        "# GET CSV"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xvyFSTUp37gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lwx0FCy3qQ58"
      },
      "outputs": [],
      "source": [
        "\n",
        "queries=['What are passenger behavior problems reported to ASRS?',\n",
        "'What is significance of reporting to the ASRS? ',\n",
        "'Tell me about Special reporting forms ?',\n",
        "'State importance of risk management in high reliability systems ',\n",
        "'Why did the ratio of cabin crew-related reports to database reports increase?',\n",
        "'What is a top priority for all participants in aviation operations?',\n",
        "]\n",
        "\n",
        "all_docs_dir = '/content/Text/'\n",
        "#\"/content/drive/MyDrive/AviationDocs\"\n",
        "df = get_csv(queries, all_docs_dir)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save to excel"
      ],
      "metadata": {
        "id": "5uYXJK_YZAse"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3VlPEsV3msN"
      },
      "outputs": [],
      "source": [
        "df.to_excel(\"Results_AviationDoc.xlsx\", index=None)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}